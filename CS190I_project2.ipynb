{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional\n",
        "from collections import Counter\n",
        "import os\n",
        "import csv\n",
        "!pip install torchmetrics\n",
        "!pip install pytorch-metric-learning\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "!pip install pytorch-lightning\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.449032Z",
          "iopub.execute_input": "2022-02-17T08:25:00.449347Z",
          "iopub.status.idle": "2022-02-17T08:25:00.482131Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.449262Z",
          "shell.execute_reply": "2022-02-17T08:25:00.481441Z"
        },
        "trusted": true,
        "id": "SWWjBbIZ5LSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965b18b4-84a3-4da6-e196-77b49b81227d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-metric-learning in /usr/local/lib/python3.9/dist-packages (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.5.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.9/dist-packages (2.0.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (2023.3.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (0.8.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (0.11.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.27.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 1\n",
        "Complete the implementation of the encode method of the Tokenizer class:\n",
        "\n",
        "`encode`: encode a given space-separated text into list of token ids according to the `self.token2idx` property. For tokens not present in the mapping, use the id of the `<unk>` token. If `max_length` is set, pad the input to `max_length` if it is less than `max_length` and truncate to `max_length` if it exceeds the length.\n",
        "\n",
        "Examples\n",
        "```python\n",
        "text = \"hello transformers !\"\n",
        "tokenizer.encode(text)                  # example output: [3, 4, 5]\n",
        "tokenizer.encode(text, max_length=5)    # example output: [3, 4, 5, 0, 0]\n",
        "tokenizer.encode(text, max_length=2)    # example output: [3, 4]\n",
        "```"
      ],
      "metadata": {
        "id": "FF4glogI5LSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        # two special tokens for padding and unknown\n",
        "        self.token2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        self.idx2token = [\"<pad>\", \"<unk>\"]\n",
        "        self.is_fit = False\n",
        "    \n",
        "    @property\n",
        "    def pad_id(self):\n",
        "        return self.token2idx[\"<pad>\"]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "    \n",
        "    def fit(self, train_texts: List[str]):\n",
        "        counter = Counter()\n",
        "        for text in train_texts:\n",
        "            counter.update(text.lower().split())\n",
        "        \n",
        "        # manually set a vocabulary size for the data set\n",
        "        vocab_size = 20000\n",
        "        self.idx2token.extend([token for token, count in counter.most_common(vocab_size - 2)])\n",
        "        for (i, token) in enumerate(self.idx2token):\n",
        "            self.token2idx[token] = i\n",
        "            \n",
        "        self.is_fit = True\n",
        "                \n",
        "\n",
        "    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n",
        "        if not self.is_fit:\n",
        "            raise Exception(\"Please fit the tokenizer on the training tokens\")\n",
        "        # 将文本转换为小写，然后分割成单词\n",
        "        words = text.lower().split() \n",
        "        # 将单词转换为对应的ID\n",
        "        ids = []\n",
        "        for word in words:\n",
        "          if word in self.token2idx:\n",
        "              ids.append(self.token2idx[word])\n",
        "          else:\n",
        "              ids.append(self.token2idx[\"<unk>\"])\n",
        "        # 将序列截断或填充到指定的长度\n",
        "        if max_length is not None:\n",
        "          if len(ids) > max_length:\n",
        "            ids = ids[:max_length]\n",
        "          else:  \n",
        "            ids = ids + [self.pad_id] * (max_length - len(ids))\n",
        "        return ids\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.483609Z",
          "iopub.execute_input": "2022-02-17T08:25:00.483925Z",
          "iopub.status.idle": "2022-02-17T08:25:00.494599Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.483881Z",
          "shell.execute_reply": "2022-02-17T08:25:00.493679Z"
        },
        "trusted": true,
        "id": "u29mNAdI5LSl"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_raw_data(filepath: str, with_tags: bool = True):\n",
        "    data = {'text': []}\n",
        "    if with_tags:\n",
        "        data['tags'] = []\n",
        "        with open(filepath) as f:\n",
        "            reader = csv.reader(f)\n",
        "            for text, tags in reader:\n",
        "                data['text'].append(text)\n",
        "                data['tags'].append(tags)\n",
        "    else:\n",
        "        with open(filepath) as f:\n",
        "            for line in f:\n",
        "                data['text'].append(line.strip())\n",
        "    return data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.495804Z",
          "iopub.execute_input": "2022-02-17T08:25:00.496201Z",
          "iopub.status.idle": "2022-02-17T08:25:00.504688Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.496166Z",
          "shell.execute_reply": "2022-02-17T08:25:00.503898Z"
        },
        "trusted": true,
        "id": "7lHbdxRn5LSm"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#手动上传data到colab"
      ],
      "metadata": {
        "id": "94u1vPV-lbXf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "# 定义数据库路径\n",
        "data_dir = \"/content/\"\n",
        "\n",
        "train_raw = load_raw_data(os.path.join(data_dir, \"train.csv\"))\n",
        "val_raw = load_raw_data(os.path.join(data_dir, \"val.csv\"))\n",
        "test_raw = load_raw_data(os.path.join(data_dir, \"test_tokens.txt\"), with_tags=False)\n",
        "# fit the tokenizer on the training tokens\n",
        "tokenizer.fit(train_raw['text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.517671Z",
          "iopub.execute_input": "2022-02-17T08:25:00.518378Z",
          "iopub.status.idle": "2022-02-17T08:25:00.795602Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.518340Z",
          "shell.execute_reply": "2022-02-17T08:25:00.794913Z"
        },
        "trusted": true,
        "id": "dEuoJh1Q5LSn"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset: \n",
        "    tag2idx = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, 'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8, 'I-MISC': 9}\n",
        "    idx2tag = ['<pad>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG','B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "  \n",
        "    def __init__(self, raw_data: Dict[str, List[str]], tokenizer: Tokenizer, max_length: int = 128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_ids = []\n",
        "        self.tag_ids = []\n",
        "        self.with_tags = False\n",
        "        for text in raw_data['text']:\n",
        "            self.token_ids.append(self.tokenizer.encode(text, max_length=max_length))\n",
        "        if 'tags' in raw_data:\n",
        "            self.with_tags = True\n",
        "            for tags in raw_data['tags']:\n",
        "                self.tag_ids.append(self.encode_tags(tags, max_length=max_length))\n",
        "    \n",
        "    def encode_tags(self, tags: str, max_length: Optional[int] = None):\n",
        "        tag_ids = [self.tag2idx[tag] for tag in tags.split()]\n",
        "        if max_length is None:\n",
        "            return tag_ids\n",
        "        # truncate the tags if longer than max_length\n",
        "        if len(tag_ids) > max_length:\n",
        "            return tag_ids[:max_length]\n",
        "        # pad with 0s if shorter than max_length\n",
        "        else:\n",
        "            return tag_ids + [0] * (max_length - len(tag_ids))  # 0 as padding for tags\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        token_ids = torch.LongTensor(self.token_ids[idx])\n",
        "        mask = token_ids == self.tokenizer.pad_id  # padding tokens\n",
        "        if self.with_tags:\n",
        "            # for training and validation\n",
        "            return token_ids, mask, torch.LongTensor(self.tag_ids[idx])\n",
        "        else:\n",
        "            # for testing\n",
        "            return token_ids, mask\n",
        "        "
      ],
      "metadata": {
        "id": "KzUsGMealyZb"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_data = NERDataset(train_raw, tokenizer)\n",
        "va_data = NERDataset(val_raw, tokenizer)\n",
        "te_data = NERDataset(test_raw, tokenizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:02.109558Z",
          "iopub.execute_input": "2022-02-17T08:25:02.109921Z",
          "iopub.status.idle": "2022-02-17T08:25:02.467151Z",
          "shell.execute_reply.started": "2022-02-17T08:25:02.109883Z",
          "shell.execute_reply": "2022-02-17T08:25:02.466435Z"
        },
        "trusted": true,
        "id": "0kMIKu-p5LSo"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 2\n",
        "Implement and experiment with transformer models. The implementation should include **at least** the following:\n",
        "- `nn.Embedding` layer to embed input token ids to the embedding space\n",
        "- `nn.TransformerEncoder` layer to perform transformer operations\n",
        "- `nn.Linear` layer as the output layer to map the output to the number of classes\n",
        "\n",
        "As we will be using the cross-entropy loss, an `nn.Softmax` or `nn.LogSoftmax` layer is not needed.\n",
        "\n",
        "You can refer to the following links for transformer Docs and examples:\n",
        "\n",
        "https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "You can modify the `__init__` method including the signature needed. For the `forward` method, the method signature is given as follows:\n",
        "\n",
        "- `src`: a `torch.LongTensor` of shape (batch_size, max_length, vocab_size) representing the input text tokens.\n",
        "\n",
        "- `src_mask`: a `torch.BoolTensor` of shape (batch_size, max_length) indicating whether an input position is padded. This is needed to prevent the transformer model attending to padded tokens.\n",
        "\n",
        "The output from the `forward` method should be of shape (batch_size, max_length, num_classes). Note that the number of classes should be 10 instead of 9 because of an additional padding class.\n"
      ],
      "metadata": {
        "id": "QVOHqRsD5LSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(embed_size, num_heads, hidden_size), num_layers)\n",
        "        self.fc = nn.Linear(embed_size, 10) # 10 classes including padding\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src_embed = self.embedding(src)\n",
        "        src_embed = src_embed.permute(1,0,2) # (max_length, batch_size, embed_size)\n",
        "        output = self.transformer_encoder(src_embed, src_key_padding_mask=src_mask)\n",
        "        output = output.permute(1,0,2) # (batch_size, max_length, embed_size)\n",
        "        output = self.fc(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:02.468580Z",
          "iopub.execute_input": "2022-02-17T08:25:02.468821Z",
          "iopub.status.idle": "2022-02-17T08:25:02.478006Z",
          "shell.execute_reply.started": "2022-02-17T08:25:02.468786Z",
          "shell.execute_reply": "2022-02-17T08:25:02.477246Z"
        },
        "trusted": true,
        "id": "Jvy2pBQL5LSo"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #modify as required\n",
        "def validate(\n",
        "    model: nn.Module, \n",
        "    dataloader: DataLoader, \n",
        "    device: torch.device,\n",
        "):\n",
        "    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            # output shape: (batch_size, max_length, num_classes)\n",
        "            logits = model(input_ids, input_mask)\n",
        "            # ignore padding index 0 when calculating loss\n",
        "            loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n",
        "                \n",
        "            loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n",
        "            is_active = torch.logical_not(input_mask)  # non-padding elements\n",
        "            # only consider non-padded tokens when calculating accuracy\n",
        "            acc_metric.update(logits[is_active], tags[is_active])\n",
        "    \n",
        "    acc = acc_metric.compute()\n",
        "    print(f\"| Validate | loss {loss_metric.compute():.4f} | acc {acc:.4f} |\")\n",
        "    return acc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:03.082423Z",
          "iopub.execute_input": "2022-02-17T08:25:03.082694Z",
          "iopub.status.idle": "2022-02-17T08:25:03.091306Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.082660Z",
          "shell.execute_reply": "2022-02-17T08:25:03.090373Z"
        },
        "trusted": true,
        "id": "Y5Eaibzu5LSp"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modify as required\n",
        "def train(\n",
        "    model: nn.Module, \n",
        "    dataloader: DataLoader, \n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "):\n",
        "    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n",
        "    model.train()\n",
        "    \n",
        "    # loop through all batches in the training\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # output shape: (batch_size, max_length, num_classes)\n",
        "        logits = model(input_ids, input_mask)\n",
        "        # ignore padding index 0 when calculating loss\n",
        "        loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n",
        "        is_active = torch.logical_not(input_mask)  # non-padding elements\n",
        "        # only consider non-padded tokens when calculating accuracy\n",
        "        acc_metric.update(logits[is_active], tags[is_active])\n",
        "    \n",
        "    print(f\"| Epoch {epoch} | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")\n",
        "    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:03.092754Z",
          "iopub.execute_input": "2022-02-17T08:25:03.093232Z",
          "iopub.status.idle": "2022-02-17T08:25:03.104319Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.093195Z",
          "shell.execute_reply": "2022-02-17T08:25:03.103543Z"
        },
        "trusted": true,
        "id": "qQtTOXRA5LSp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modify as required\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# data loaders\n",
        "train_dataloader = DataLoader(tr_data, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(va_data, batch_size=16)\n",
        "test_dataloader = DataLoader(te_data, batch_size=16)\n",
        "\n",
        "# move the model to device\n",
        "model = TransformerModel(vocab_size = len(tokenizer), \n",
        "    embed_size = 256, \n",
        "    num_heads = 4, \n",
        "    hidden_size = 256,\n",
        "    num_layers = 2,).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(20):\n",
        "    train(model, train_dataloader, optimizer, device, epoch)\n",
        "validate(model, val_dataloader, device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:03.105602Z",
          "iopub.execute_input": "2022-02-17T08:25:03.105982Z",
          "iopub.status.idle": "2022-02-17T08:25:43.205981Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.105946Z",
          "shell.execute_reply": "2022-02-17T08:25:43.205084Z"
        },
        "trusted": true,
        "id": "5Be4ZCs15LSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c389b669-5b71-47bd-cc22-e6af3ac6917c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:52<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 0 | loss 0.4305 | acc 0.8856 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:50<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 1 | loss 0.2405 | acc 0.9297 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:48<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 2 | loss 0.1702 | acc 0.9484 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:48<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 3 | loss 0.1335 | acc 0.9587 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:48<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 4 | loss 0.1115 | acc 0.9647 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:48<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 5 | loss 0.0968 | acc 0.9689 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:48<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 6 | loss 0.0857 | acc 0.9727 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:48<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 7 | loss 0.0777 | acc 0.9750 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:48<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 8 | loss 0.0705 | acc 0.9769 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:48<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 9 | loss 0.0693 | acc 0.9778 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:47<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 10 | loss 0.0643 | acc 0.9793 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:50<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 11 | loss 0.0605 | acc 0.9804 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:50<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 12 | loss 0.0570 | acc 0.9814 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:51<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 13 | loss 0.0545 | acc 0.9823 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:51<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 14 | loss 0.0536 | acc 0.9828 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:51<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 15 | loss 0.0515 | acc 0.9829 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:51<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 16 | loss 0.0488 | acc 0.9842 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:52<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 17 | loss 0.0444 | acc 0.9856 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:52<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 18 | loss 0.0455 | acc 0.9851 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [04:52<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 19 | loss 0.0416 | acc 0.9864 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:18<00:00, 11.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Validate | loss 0.3429 | acc 0.9266 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9266)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()"
      ],
      "metadata": {
        "id": "LykWtTusAwRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 3\n",
        "Make predictions on the validation data and evaluate entity-level F1 scores using conlleval script.\n",
        "\n",
        "`predict`: taking inputs of a trained model, a dataloader, and a torch device, predict the tags for all tokens in the data set. The output should be a nested list of lists, each containing tag predictions for a single sentence.\n",
        "\n",
        "    Input texts in the dataloader (2 sentences):\n",
        "    EU rejects German call\n",
        "    Only France and Britain backed Fischler 's proposal .\n",
        "    \n",
        "    Example output:\n",
        "    [['B-ORG', 'O', 'B-MISC', 'O'], ['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']]\n",
        "        "
      ],
      "metadata": {
        "id": "uQV7JhRl5LSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement the predict function\n",
        "\n",
        "def predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            input_ids, input_mask = batch[0].to(device), batch[1].to(device)\n",
        "            # output shape: (batch_size, max_length, num_classes)\n",
        "            logits = model(input_ids, input_mask)\n",
        "            # apply softmax to obtain probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # find the predicted class labels (indices)\n",
        "            pred_indices = torch.argmax(probs, dim=-1).cpu().numpy()\n",
        "\n",
        "            # convert the predicted indices back to their corresponding tags and ignore padding tokens\n",
        "            for i, seq in enumerate(pred_indices):\n",
        "                preds.append([NERDataset.idx2tag[idx] for idx in seq[:sum(~input_mask[i].cpu().numpy())]])\n",
        "                    \n",
        "    return preds\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:43.207551Z",
          "iopub.execute_input": "2022-02-17T08:25:43.207812Z",
          "iopub.status.idle": "2022-02-17T08:25:43.426052Z",
          "shell.execute_reply.started": "2022-02-17T08:25:43.207774Z",
          "shell.execute_reply": "2022-02-17T08:25:43.425331Z"
        },
        "trusted": true,
        "id": "2BeTuu4i5LSq"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2idx = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, 'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8, 'I-MISC': 9}"
      ],
      "metadata": {
        "id": "FG4k0H6RdnQ-"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
        "from conlleval import evaluate"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:43.430210Z",
          "iopub.execute_input": "2022-02-17T08:25:43.431127Z",
          "iopub.status.idle": "2022-02-17T08:25:44.519771Z",
          "shell.execute_reply.started": "2022-02-17T08:25:43.431084Z",
          "shell.execute_reply": "2022-02-17T08:25:44.519005Z"
        },
        "trusted": true,
        "id": "EzFjEe0c5LSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d377b0-39a0-4295-b1ef-a1b892cdc132"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 06:02:04--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: ‘conlleval.py.4’\n",
            "\n",
            "\rconlleval.py.4        0%[                    ]       0  --.-KB/s               \rconlleval.py.4      100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-21 06:02:04 (44.7 MB/s) - ‘conlleval.py.4’ saved [7502/7502]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use the conlleval script to measure the entity-level f1\n",
        "pred_tags = []\n",
        "for tags in predict(model, val_dataloader, device):\n",
        "    pred_tags.extend(tags)\n",
        "    pred_tags.append('O')\n",
        "    \n",
        "true_tags = []\n",
        "for tags in val_raw['tags']:\n",
        "    true_tags.extend(tags.strip().split())\n",
        "    true_tags.append('O')\n",
        "\n",
        "evaluate(true_tags, pred_tags)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:44.521224Z",
          "iopub.execute_input": "2022-02-17T08:25:44.521511Z",
          "iopub.status.idle": "2022-02-17T08:25:45.394177Z",
          "shell.execute_reply.started": "2022-02-17T08:25:44.521470Z",
          "shell.execute_reply": "2022-02-17T08:25:45.393518Z"
        },
        "trusted": true,
        "id": "lVAnQYdD5LSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a94acdf-b8de-4e51-aa66-f576b051fd9c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:19<00:00, 10.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 54612 tokens with 5942 phrases; found: 5802 phrases; correct: 3843.\n",
            "accuracy:  63.14%; (non-O)\n",
            "accuracy:  93.10%; precision:  66.24%; recall:  64.68%; FB1:  65.45\n",
            "              LOC: precision:  80.06%; recall:  78.44%; FB1:  79.24  1800\n",
            "             MISC: precision:  73.16%; recall:  72.13%; FB1:  72.64  909\n",
            "              ORG: precision:  54.58%; recall:  59.51%; FB1:  56.94  1462\n",
            "              PER: precision:  57.57%; recall:  50.98%; FB1:  54.07  1631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66.23578076525337, 64.67519353752945, 65.44618528610356)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example output from the above codeblock. We will take the overall test F1 score (69.24 in this example) and grade accordingly.\n",
        "```\n",
        "processed 54612 tokens with 5942 phrases; found: 5554 phrases; correct: 3980.\n",
        "accuracy:  65.78%; (non-O)\n",
        "accuracy:  93.88%; precision:  71.66%; recall:  66.98%; FB1:  69.24\n",
        "              LOC: precision:  84.58%; recall:  77.03%; FB1:  80.63  1673\n",
        "             MISC: precision:  77.31%; recall:  71.69%; FB1:  74.40  855\n",
        "              ORG: precision:  58.71%; recall:  63.83%; FB1:  61.16  1458\n",
        "              PER: precision:  66.84%; recall:  56.89%; FB1:  61.47  1568\n",
        "(71.66006481814908, 66.98081454055873, 69.24147529575504)\n",
        "```\n",
        "If the codeblock above errors out, check your implementation of the `predict` function. It should return a nested list of lists, each containing predicted tags in their IOB string forms."
      ],
      "metadata": {
        "id": "ztKqd9J15LSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 4\n",
        "Once you finish all previous todos and are satisfied with the model performance on the validation set, make predictions on the test set and keep a copy of the `submission.txt` file by downloading it to your local machine. You can find `submission.txt` under Output > `/kaggle/working`."
      ],
      "metadata": {
        "id": "IaChyXkY5LSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOU SHOULD NOT CHANGE THIS CODEBLOCK\n",
        "# make prediction on the test set and save to submission.txt\n",
        "preds = predict(model, val_dataloader, device)\n",
        "with open(\"submission.txt\", \"w\") as f:\n",
        "    for tags in preds:\n",
        "        f.write(\" \".join(tags) + \"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:45.395550Z",
          "iopub.execute_input": "2022-02-17T08:25:45.395822Z",
          "iopub.status.idle": "2022-02-17T08:25:46.111937Z",
          "shell.execute_reply.started": "2022-02-17T08:25:45.395787Z",
          "shell.execute_reply": "2022-02-17T08:25:46.111234Z"
        },
        "trusted": true,
        "id": "dVt102qy5LSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515fcbdc-3986-4447-f7b8-81ccb69a6dcf"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:19<00:00, 10.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:27:24.950886Z",
          "iopub.execute_input": "2022-02-17T08:27:24.951166Z",
          "iopub.status.idle": "2022-02-17T08:27:24.957143Z",
          "shell.execute_reply.started": "2022-02-17T08:27:24.951135Z",
          "shell.execute_reply": "2022-02-17T08:27:24.956414Z"
        },
        "trusted": true,
        "id": "jJOtOKyR5LSs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45984859-9ebc-4583-a728-df140b876faf"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:27:30.109566Z",
          "iopub.execute_input": "2022-02-17T08:27:30.109841Z",
          "iopub.status.idle": "2022-02-17T08:27:30.796831Z",
          "shell.execute_reply.started": "2022-02-17T08:27:30.109811Z",
          "shell.execute_reply": "2022-02-17T08:27:30.795875Z"
        },
        "trusted": true,
        "id": "5VquJfjq5LSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24587871-acc2-4e80-a210-ece1f66a5b0f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conlleval.py    conlleval.py.3  \u001b[0m\u001b[01;34msample_data\u001b[0m/     train.csv\n",
            "conlleval.py.1  conlleval.py.4  submission.txt   val.csv\n",
            "conlleval.py.2  \u001b[01;34m__pycache__\u001b[0m/    test_tokens.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ioUQaKri5LSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}